\newenvironment{abstractpage}
{\cleardoublepage\vspace*{\fill}\thispagestyle{empty}}
{\vfill\cleardoublepage}
\newenvironment{myabstract}[1]
{\bigskip\selectlanguage{#1}
  \begin{center}
    \bfseries\abstractname
  \end{center}}
{\par\bigskip}

\begin{abstractpage}
\begin{myabstract}{german}
In dieser Arbeit geht es um den Entwicklungsprozess einer Anwendung
für mobiles Maschinelles Lernen. Zusammen mit Google Coral Edge TPUs
(\textit{Tensor Processing Units})
wird die Architektur eines Systems vorgestellt, mit dem
neuronale Netze auf verteilten Geräten ausgeführt
und Netzergebnisse kommuniziert werden.
Hierfür wird damit begonnen, die Geschichte der Künstlichen Intelligenz
zu untersuchen, um anschließend einen Einstieg in das Gebiet
von Deep Learning zu geben. Mithilfe von TensorFlow wird
gezeigt, wie ein neuronales Netz trainiert
und auf mobilen und eingebetteten Geräten bereitgestellt wird.
Des Weiteren wird hierbei die Integration mit der Edge TPU besprochen
und Testergebnisse in Bezug auf Genauigkeit und Inferenzgeschwindigkeit
dokumentiert. Die Ausführung auf der Edge TPU erzeugt keine
oder nur kaum bemerkbare Genauigkeitsverluste, jedoch ist die Geschwindigkeit
in den durchgeführten Benchmarks
langsamer als die gleiche Ausführung auf der CPU.
Mögliche Auslöser für dieses Ergebnis können schließlich diskutiert werden.
\end{myabstract}
\begin{myabstract}{english}
This thesis deals with the development process of building
an application for mobile Machine Learning.
Using Google Coral Edge TPUs (tensor processing units)
we describe a distributed system
for querying neural networks wrapped in a service.
We will start by examining the history of Artificial Intelligence
and then give an introduction into the field of Deep Learning.
With the help of TensorFlow, we show the process of training a
neural network and how to deploy the model to
mobile and embedded devices. Moreover, both CPU and TPU models are compared.
Inference on the Edge TPU does not reduce the model's accuracy.
However, when benchmarking the performance,
the model performed slower than when running on the CPU.
Finally, we discuss possible triggers for this behaviour.
\end{myabstract}
\end{abstractpage}